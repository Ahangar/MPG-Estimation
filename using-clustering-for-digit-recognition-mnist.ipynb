{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-12T23:32:28.733737Z","iopub.execute_input":"2025-03-12T23:32:28.734063Z","iopub.status.idle":"2025-03-12T23:32:28.740666Z","shell.execute_reply.started":"2025-03-12T23:32:28.734042Z","shell.execute_reply":"2025-03-12T23:32:28.739911Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1- Objective\n\nDigit recognition is essential for enabling machines to interpret handwritten or printed numbers, improving efficiency in fields like finance and healthcare. Machine learning models, such as Convolutional Neural Networks (CNNs), are particularly effective for this task due to their ability to detect spatial patterns in images. However, in this work we try to use a more novel clustering approach and explore the limitations of it.","metadata":{}},{"cell_type":"markdown","source":"# 2- Dataset\n\nThe famous MNIST dataset is used here. MNIST is a subset of a larger set available from NIST (it's copied from http://yann.lecun.com/exdb/mnist/).\nThe MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\ntest = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T22:08:15.485344Z","iopub.execute_input":"2025-03-12T22:08:15.485629Z","iopub.status.idle":"2025-03-12T22:08:17.265171Z","shell.execute_reply.started":"2025-03-12T22:08:15.485604Z","shell.execute_reply":"2025-03-12T22:08:17.264358Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Each row of the data represents the values in each pixel of 28x28 image. As you can see there are about equal number of training datapoints for each label (number).","metadata":{}},{"cell_type":"code","source":"plt.hist(train.label);\nplt.xlabel('Number')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T22:08:17.265780Z","iopub.execute_input":"2025-03-12T22:08:17.265984Z","iopub.status.idle":"2025-03-12T22:08:17.379946Z","shell.execute_reply.started":"2025-03-12T22:08:17.265963Z","shell.execute_reply":"2025-03-12T22:08:17.379143Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's try to visualize a sample image.","metadata":{}},{"cell_type":"code","source":"def plot_image(image_1d):\n    plt.imshow(image_1d.values.reshape(28,28))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T22:08:17.380669Z","iopub.execute_input":"2025-03-12T22:08:17.380940Z","iopub.status.idle":"2025-03-12T22:08:17.384260Z","shell.execute_reply.started":"2025-03-12T22:08:17.380917Z","shell.execute_reply":"2025-03-12T22:08:17.383520Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_image(train.iloc[42,1:])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T22:08:17.385884Z","iopub.execute_input":"2025-03-12T22:08:17.386053Z","iopub.status.idle":"2025-03-12T22:08:17.505772Z","shell.execute_reply.started":"2025-03-12T22:08:17.386037Z","shell.execute_reply":"2025-03-12T22:08:17.504930Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3- Data Preparation\n## 3-1- Normalization\nLet's start by normalizing the data based on the largest value:","metadata":{}},{"cell_type":"code","source":"def norm_half(x):\n    return(x/255)\n\ntrain_pix_01 = train.iloc[:,1:].applymap(norm_half); #these are pixel values between 0 and 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T23:32:33.361720Z","iopub.execute_input":"2025-03-12T23:32:33.362049Z","iopub.status.idle":"2025-03-12T23:32:38.595309Z","shell.execute_reply.started":"2025-03-12T23:32:33.362026Z","shell.execute_reply":"2025-03-12T23:32:38.594576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_image(train_pix_01.iloc[42,:])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T22:08:22.585422Z","iopub.execute_input":"2025-03-12T22:08:22.585696Z","iopub.status.idle":"2025-03-12T22:08:22.703655Z","shell.execute_reply.started":"2025-03-12T22:08:22.585673Z","shell.execute_reply":"2025-03-12T22:08:22.702970Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3-2- Train Test Split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T22:08:41.866366Z","iopub.execute_input":"2025-03-12T22:08:41.866666Z","iopub.status.idle":"2025-03-12T22:08:41.870117Z","shell.execute_reply.started":"2025-03-12T22:08:41.866638Z","shell.execute_reply":"2025-03-12T22:08:41.869031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train_pix_01,\n                                                   train.label,random_state = 42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T22:08:42.188340Z","iopub.execute_input":"2025-03-12T22:08:42.188647Z","iopub.status.idle":"2025-03-12T22:08:43.063962Z","shell.execute_reply.started":"2025-03-12T22:08:42.188626Z","shell.execute_reply":"2025-03-12T22:08:43.063238Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4- Clustering\nHere we try two different clustering methods for categorizing different digits:\n- KMeans\n- Agglomerative Clustering","metadata":{}},{"cell_type":"markdown","source":"## 4-1- KMeans\nWe start by the KMeans algorithm. Since we know that we have 10 digits, we can start by setting 10 for the number of clusters.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T22:08:44.972366Z","iopub.execute_input":"2025-03-12T22:08:44.972651Z","iopub.status.idle":"2025-03-12T22:08:44.975920Z","shell.execute_reply.started":"2025-03-12T22:08:44.972630Z","shell.execute_reply":"2025-03-12T22:08:44.975009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"km = KMeans(n_clusters =10, random_state = 42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T23:01:11.144427Z","iopub.execute_input":"2025-03-12T23:01:11.144745Z","iopub.status.idle":"2025-03-12T23:01:11.148381Z","shell.execute_reply.started":"2025-03-12T23:01:11.144723Z","shell.execute_reply":"2025-03-12T23:01:11.147597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"km.fit(X_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T23:32:38.596169Z","iopub.execute_input":"2025-03-12T23:32:38.596329Z","iopub.status.idle":"2025-03-12T23:33:07.653708Z","shell.execute_reply.started":"2025-03-12T23:32:38.596314Z","shell.execute_reply":"2025-03-12T23:33:07.652967Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, we have to look at our clusters. We plot each cluster and also a histogram of the labels used in each of them.","metadata":{}},{"cell_type":"code","source":"# Create subplots\nfig, axes = plt.subplots(10, 2, figsize=(6, 40))  # 2 rows, 2 columns\n\nfor i in range(10):\n    axes[i][0].imshow(X_train[km.labels_ == i].mean().values.reshape(28,28))\n    y_train[km.labels_ == i].hist( ax = axes[i][1])\n    axes[i][1].set_title('For this cluster the most common number is ' +\n                 str (y_train[km.labels_ == i].mode().values[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T22:09:05.406802Z","iopub.execute_input":"2025-03-12T22:09:05.406972Z","iopub.status.idle":"2025-03-12T22:09:08.136260Z","shell.execute_reply.started":"2025-03-12T22:09:05.406957Z","shell.execute_reply":"2025-03-12T22:09:08.135373Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It looks like the clustering algorithm does a good job for clustering numbers 2, 1, 0, and 6. It does worse for numbers 3, 4, 8, and 7 and there is no cluster with majority labels of 9 and 5.\nMaybe we can improve it by increasing the number of clusters using some inertia L plots to find the best number of K.","metadata":{}},{"cell_type":"code","source":"km_list = list()\n\nfor clust in range(1,32,4):\n    km = KMeans(n_clusters=clust, random_state=42)\n    km = km.fit(X_train)\n    \n    km_list.append(pd.Series({'clusters': clust, \n                              'inertia': km.inertia_,\n                              'model': km}))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T23:33:07.655054Z","iopub.execute_input":"2025-03-12T23:33:07.655293Z","iopub.status.idle":"2025-03-12T23:35:54.125044Z","shell.execute_reply.started":"2025-03-12T23:33:07.655275Z","shell.execute_reply":"2025-03-12T23:35:54.124136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_data = (pd.concat(km_list, axis=1)\n             .T\n             [['clusters','inertia']]\n             .set_index('clusters'))\n\nax = plot_data.plot(marker='o',ls='-')\nax.set_xticks(range(1,32,4))\nax.set_xlim(0,8)\nax.set(xlabel='Cluster', ylabel='Inertia');","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T21:52:38.469654Z","iopub.execute_input":"2025-03-12T21:52:38.469931Z","iopub.status.idle":"2025-03-12T21:52:38.597251Z","shell.execute_reply.started":"2025-03-12T21:52:38.469909Z","shell.execute_reply":"2025-03-12T21:52:38.596477Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The inertial plot does not look as helpful as we wanted. There is no sudden drop. Let's just try a larger K than 10 and see what happens:","metadata":{}},{"cell_type":"code","source":"km = KMeans(n_clusters =18, random_state = 42)\nkm.fit(X_train);","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T00:03:47.227993Z","iopub.execute_input":"2025-03-13T00:03:47.228291Z","iopub.status.idle":"2025-03-13T00:04:16.918271Z","shell.execute_reply.started":"2025-03-13T00:03:47.228272Z","shell.execute_reply":"2025-03-13T00:04:16.917674Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create subplots\nfig, axes = plt.subplots(18, 2, figsize=(6, 30))  # 2 rows, 2 columns\n\n# Flatten axes array for easy iteration\n#axes = axes.flatten()\n\nfor i in range(18):\n    axes[i][0].imshow(X_train[km.labels_ == i].mean().values.reshape(28,28))\n    y_train[km.labels_ == i].hist( ax = axes[i][1])\n    #axes[i][1].set_title('For this cluster the most common number is ' +\n               #  str (y_train[km.labels_ == i].mode().values[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T22:04:49.148218Z","iopub.execute_input":"2025-03-12T22:04:49.148382Z","iopub.status.idle":"2025-03-12T22:04:52.372584Z","shell.execute_reply.started":"2025-03-12T22:04:49.148367Z","shell.execute_reply":"2025-03-12T22:04:52.371827Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This looks like it is doing a better job. However, we there are still issues for detection between.\nWe'll stop here as more investigation is outside the scope here. ","metadata":{}},{"cell_type":"markdown","source":"## 4-2- Hierarchical Clustering\n\nWe can use Agglomerative Clustering as the second method here. One issue is that this type of clustering is slow so we are not going to try different K's. Let's just use the K from previous part.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\n### BEGIN SOLUTION\nag = AgglomerativeClustering(n_clusters=18, linkage='ward', compute_full_tree=True)\nag.fit(X_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T22:35:25.819911Z","iopub.execute_input":"2025-03-12T22:35:25.820250Z","iopub.status.idle":"2025-03-12T22:39:01.088072Z","shell.execute_reply.started":"2025-03-12T22:35:25.820225Z","shell.execute_reply":"2025-03-12T22:39:01.086947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create subplots\nfig, axes = plt.subplots(18, 2, figsize=(6, 30))  # 2 rows, 2 columns\n\nfor i in range(18):\n    axes[i][0].imshow(X_train[ag.labels_ == i].mean().values.reshape(28,28))\n    y_train[ag.labels_ == i].hist( ax = axes[i][1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T22:28:01.634208Z","iopub.execute_input":"2025-03-12T22:28:01.634469Z","iopub.status.idle":"2025-03-12T22:28:05.056756Z","shell.execute_reply.started":"2025-03-12T22:28:01.634443Z","shell.execute_reply":"2025-03-12T22:28:05.055876Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4- Performance\n\nWe usually compare the performance of two models by checking the test set. Unfortunately, we cannot do the same for the hierarchical clustering algorithm since it cannot predict labels for new observations. So we compare the results of two algorithm for the train set and also compute the performance of KMeans in the test set sepearately  Let's assign the most common number to each label.  ","metadata":{}},{"cell_type":"markdown","source":"## 4-1- Comparision\nWe use accuracy score for comparing the models.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T22:44:07.623413Z","iopub.execute_input":"2025-03-12T22:44:07.623761Z","iopub.status.idle":"2025-03-12T22:44:07.627231Z","shell.execute_reply.started":"2025-03-12T22:44:07.623740Z","shell.execute_reply":"2025-03-12T22:44:07.626363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_train_km = [y_train[km.labels_ == x].mode()[0] for x in km.labels_]\ny_pred_train_ag = [y_train[ag.labels_ == x].mode()[0] for x in ag.labels_]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T22:44:18.203613Z","iopub.execute_input":"2025-03-12T22:44:18.203881Z","iopub.status.idle":"2025-03-12T22:44:34.485978Z","shell.execute_reply.started":"2025-03-12T22:44:18.203864Z","shell.execute_reply":"2025-03-12T22:44:34.485201Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#KMeansClustering\naccuracy_score(y_train,y_pred_train_km)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T23:03:39.338854Z","iopub.execute_input":"2025-03-12T23:03:39.339143Z","iopub.status.idle":"2025-03-12T23:03:39.349976Z","shell.execute_reply.started":"2025-03-12T23:03:39.339122Z","shell.execute_reply":"2025-03-12T23:03:39.349253Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#AgglomerativeClustering\naccuracy_score(y_train,y_pred_train_ag)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T22:46:39.774742Z","iopub.execute_input":"2025-03-12T22:46:39.775103Z","iopub.status.idle":"2025-03-12T22:46:39.786645Z","shell.execute_reply.started":"2025-03-12T22:46:39.775071Z","shell.execute_reply":"2025-03-12T22:46:39.785753Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can see that the Agglomerative Clustering is doing a better job. However, this method lacks prediction capablities that we need for such a problem.","metadata":{}},{"cell_type":"code","source":"km_results = km.predict(X_test)\ny_pred_km = [y_train[km.labels_ == x].mode()[0] for x in km_results]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T22:29:43.774187Z","iopub.execute_input":"2025-03-12T22:29:43.774534Z","iopub.status.idle":"2025-03-12T22:29:46.841572Z","shell.execute_reply.started":"2025-03-12T22:29:43.774480Z","shell.execute_reply":"2025-03-12T22:29:46.840940Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4-2- Test Set Performance","metadata":{}},{"cell_type":"code","source":"km_results = km.predict(X_test)\ny_pred_km = [y_train[km.labels_ == x].mode()[0] for x in km_results]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T23:01:50.814699Z","iopub.execute_input":"2025-03-12T23:01:50.814924Z","iopub.status.idle":"2025-03-12T23:01:53.130284Z","shell.execute_reply.started":"2025-03-12T23:01:50.814905Z","shell.execute_reply":"2025-03-12T23:01:53.129463Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#KMeansClustering\naccuracy_score(y_test,y_pred_km)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T23:02:25.748604Z","iopub.execute_input":"2025-03-12T23:02:25.748887Z","iopub.status.idle":"2025-03-12T23:02:25.756186Z","shell.execute_reply.started":"2025-03-12T23:02:25.748868Z","shell.execute_reply":"2025-03-12T23:02:25.755537Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The performance on the test set is very similar to the train set which shows consistency in our modeling.","metadata":{}},{"cell_type":"markdown","source":"# 6- Submission","metadata":{}},{"cell_type":"code","source":"test_pix_01 = test.applymap(norm_half); #these are pixel values between 0 and 1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T00:04:16.919904Z","iopub.execute_input":"2025-03-13T00:04:16.920157Z","iopub.status.idle":"2025-03-13T00:04:20.373198Z","shell.execute_reply.started":"2025-03-13T00:04:16.920133Z","shell.execute_reply":"2025-03-13T00:04:20.372309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"km_results = km.predict(test_pix_01)\ny_pred_km = [y_train[km.labels_ == x].mode()[0] for x in km_results]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T00:04:20.374538Z","iopub.execute_input":"2025-03-13T00:04:20.374788Z","iopub.status.idle":"2025-03-13T00:04:26.550519Z","shell.execute_reply.started":"2025-03-13T00:04:20.374765Z","shell.execute_reply":"2025-03-13T00:04:26.549748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df = pd.DataFrame({\n    \"ImageId\": np.arange(1, len(y_pred_km) + 1),\n    \"Label\": y_pred_km\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T00:27:48.575595Z","iopub.execute_input":"2025-03-13T00:27:48.576375Z","iopub.status.idle":"2025-03-13T00:27:48.624279Z","shell.execute_reply.started":"2025-03-13T00:27:48.576355Z","shell.execute_reply":"2025-03-13T00:27:48.623186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T00:06:13.179325Z","iopub.execute_input":"2025-03-13T00:06:13.179618Z","iopub.status.idle":"2025-03-13T00:06:13.218237Z","shell.execute_reply.started":"2025-03-13T00:06:13.179597Z","shell.execute_reply":"2025-03-13T00:06:13.217563Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7- Findings and Flaws\n\nIn this work we used clustering algorithms to predict MNIST digit recognition. The Agglomerative clustering had better performance but lacks the capablity to predict new observations. The KMeans model had an accuracy of 0.7 for both training and testing sets. \n\nThe main flaw of this work is that while clustering can provide some interesting insights, it is not a proper algorithm for digit recognition.\n\nBoth of the algorithms struggle with some of the numbers (e.g. 4 and 9) due to similarity. The performance of both algorithm is far below simple Neural Network solutions for this problem.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}